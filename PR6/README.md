# Практическая работа 6

# Исследование вредоносной активности в домене Windows

## Цель работы

1.  Закрепить навыки исследования данных журнала Windows Active
    Directory
2.  Изучить структуру журнала системы Windows Active Directory
3.  Закрепить практические навыки использования языка программирования R
    для обработки данных
4.  Закрепить знания основных функций обработки данных экосистемы
    `tidyverse` языка R

## Исходные данные

1.  RStudio

## Задание

Используя программный пакет `dplyr` языка программирования R, провести
анализ журналов и ответить на вопросы

## Ход работы

### Шаг 1. Подготовка данных

``` r
library(dplyr)
```


    Присоединяю пакет: 'dplyr'

    Следующие объекты скрыты от 'package:stats':

        filter, lag

    Следующие объекты скрыты от 'package:base':

        intersect, setdiff, setequal, union

``` r
library(jsonlite)
library(tidyr)
library(xml2)
```

    Warning: пакет 'xml2' был собран под R версии 4.3.2

``` r
library(rvest)
```

    Warning: пакет 'rvest' был собран под R версии 4.3.2

jsonlite: Эта библиотека предоставляет функции для работы с данными в
формате JSON. Она позволяет импортировать и экспортировать данные в
формате JSON, а также выполнять операции по обработке и манипуляции с
JSON-данными.

tidyr: Эта библиотека предоставляет функции для работы с данными в
“аккуратном” формате (tidy data). Она позволяет перестраивать данные,
преобразовывать широкий формат (wide format) в длинный формат (long
format) и наоборот, а также выполнять другие операции по преобразованию
структуры данных.

xml2: Эта библиотека предоставляет функции для работы с данными в
формате XML. Она позволяет импортировать и экспортировать данные в
формате XML, а также выполнять операции по обработке и манипуляции с
XML-данными.

rvest: Эта библиотека предоставляет функции для web-скрапинга и
извлечения данных с веб-страниц. Она позволяет получать данные из
HTML-страниц, извлекать таблицы, тексты, ссылки и другую информацию с
веб-страниц, а также выполнять другие операции по обработке и анализу
веб-данных.

#### Импорт данных

``` r
url <- "https://storage.yandexcloud.net/iamcth-data/dataset.tar.gz"
download.file(url, destfile = tf <- tempfile(fileext = ".tar.gz"), mode = "wb")
temp_dir <- tempdir()
untar(tf, exdir = temp_dir)
json_files <- list.files(temp_dir, pattern="\\.json$", full.names = TRUE, recursive = TRUE)
data <- stream_in(file(json_files))
```

    opening file input connection.


     Found 500 records...
     Found 1000 records...
     Found 1500 records...
     Found 2000 records...
     Found 2500 records...
     Found 3000 records...
     Found 3500 records...
     Found 4000 records...
     Found 4500 records...
     Found 5000 records...
     Found 5500 records...
     Found 6000 records...
     Found 6500 records...
     Found 7000 records...
     Found 7500 records...
     Found 8000 records...
     Found 8500 records...
     Found 9000 records...
     Found 9500 records...
     Found 10000 records...
     Found 10500 records...
     Found 11000 records...
     Found 11500 records...
     Found 12000 records...
     Found 12500 records...
     Found 13000 records...
     Found 13500 records...
     Found 14000 records...
     Found 14500 records...
     Found 15000 records...
     Found 15500 records...
     Found 16000 records...
     Found 16500 records...
     Found 17000 records...
     Found 17500 records...
     Found 18000 records...
     Found 18500 records...
     Found 19000 records...
     Found 19500 records...
     Found 20000 records...
     Found 20500 records...
     Found 21000 records...
     Found 21500 records...
     Found 22000 records...
     Found 22500 records...
     Found 23000 records...
     Found 23500 records...
     Found 24000 records...
     Found 24500 records...
     Found 25000 records...
     Found 25500 records...
     Found 26000 records...
     Found 26500 records...
     Found 27000 records...
     Found 27500 records...
     Found 28000 records...
     Found 28500 records...
     Found 29000 records...
     Found 29500 records...
     Found 30000 records...
     Found 30500 records...
     Found 31000 records...
     Found 31500 records...
     Found 32000 records...
     Found 32500 records...
     Found 33000 records...
     Found 33500 records...
     Found 34000 records...
     Found 34500 records...
     Found 35000 records...
     Found 35500 records...
     Found 36000 records...
     Found 36500 records...
     Found 37000 records...
     Found 37500 records...
     Found 38000 records...
     Found 38500 records...
     Found 39000 records...
     Found 39500 records...
     Found 40000 records...
     Found 40500 records...
     Found 41000 records...
     Found 41500 records...
     Found 42000 records...
     Found 42500 records...
     Found 43000 records...
     Found 43500 records...
     Found 44000 records...
     Found 44500 records...
     Found 45000 records...
     Found 45500 records...
     Found 46000 records...
     Found 46500 records...
     Found 47000 records...
     Found 47500 records...
     Found 48000 records...
     Found 48500 records...
     Found 49000 records...
     Found 49500 records...
     Found 50000 records...
     Found 50500 records...
     Found 51000 records...
     Found 51500 records...
     Found 52000 records...
     Found 52500 records...
     Found 53000 records...
     Found 53500 records...
     Found 54000 records...
     Found 54500 records...
     Found 55000 records...
     Found 55500 records...
     Found 56000 records...
     Found 56500 records...
     Found 57000 records...
     Found 57500 records...
     Found 58000 records...
     Found 58500 records...
     Found 59000 records...
     Found 59500 records...
     Found 60000 records...
     Found 60500 records...
     Found 61000 records...
     Found 61500 records...
     Found 62000 records...
     Found 62500 records...
     Found 63000 records...
     Found 63500 records...
     Found 64000 records...
     Found 64500 records...
     Found 65000 records...
     Found 65500 records...
     Found 66000 records...
     Found 66500 records...
     Found 67000 records...
     Found 67500 records...
     Found 68000 records...
     Found 68500 records...
     Found 69000 records...
     Found 69500 records...
     Found 70000 records...
     Found 70500 records...
     Found 71000 records...
     Found 71500 records...
     Found 72000 records...
     Found 72500 records...
     Found 73000 records...
     Found 73500 records...
     Found 74000 records...
     Found 74500 records...
     Found 75000 records...
     Found 75500 records...
     Found 76000 records...
     Found 76500 records...
     Found 77000 records...
     Found 77500 records...
     Found 78000 records...
     Found 78500 records...
     Found 79000 records...
     Found 79500 records...
     Found 80000 records...
     Found 80500 records...
     Found 81000 records...
     Found 81500 records...
     Found 82000 records...
     Found 82500 records...
     Found 83000 records...
     Found 83500 records...
     Found 84000 records...
     Found 84500 records...
     Found 85000 records...
     Found 85500 records...
     Found 86000 records...
     Found 86500 records...
     Found 87000 records...
     Found 87500 records...
     Found 88000 records...
     Found 88500 records...
     Found 89000 records...
     Found 89500 records...
     Found 90000 records...
     Found 90500 records...
     Found 91000 records...
     Found 91500 records...
     Found 92000 records...
     Found 92500 records...
     Found 93000 records...
     Found 93500 records...
     Found 94000 records...
     Found 94500 records...
     Found 95000 records...
     Found 95500 records...
     Found 96000 records...
     Found 96500 records...
     Found 97000 records...
     Found 97500 records...
     Found 98000 records...
     Found 98500 records...
     Found 99000 records...
     Found 99500 records...
     Found 1e+05 records...
     Found 100500 records...
     Found 101000 records...
     Found 101500 records...
     Found 101904 records...
     Imported 101904 records. Simplifying...

    closing file input connection.

Выполняем чтение данных из JSON-файла и сохраняет их в переменную data.

#### Привести датасеты в вид “аккуратных данных”, преобразовать типы столбцов в соответствии с типом данных

``` r
neat_data <- data %>%
  mutate(`@timestamp` = as.POSIXct(`@timestamp`, format = "%Y-%m-%dT%H:%M:%OSZ", tz = "UTC")) %>%
  rename(timestamp = `@timestamp`, metadata = `@metadata`)
```

Преобразуем столбец @timestamp в формат POSIXct, который является типом
данных для хранения даты и времени. Функция as.POSIXct() принимает на
вход значение @timestamp, указывает формат даты и времени
“%Y-%m-%dT%H:%M:%OSZ” и устанавливает временную зону tz = “UTC”. Далее
переименовываем столбцы @timestamp и @metadata в timestamp и metadata
соответственно. Таким образом, столбцы будут иметь более понятные и
удобные для использования имена.

#### Просмотрите общую структуру данных с помощью функции glimpse()

``` r
glimpse(neat_data)
```

    Rows: 101,904
    Columns: 9
    $ timestamp <dttm> 2019-10-20 20:11:06, 2019-10-20 20:11:07, 2019-10-20 20:11:…
    $ metadata  <df[,4]> <data.frame[26 x 4]>
    $ event     <df[,4]> <data.frame[26 x 4]>
    $ log       <df[,1]> <data.frame[26 x 1]>
    $ message   <chr> "A token right was adjusted.\n\nSubject:\n\tSecurity ID:\…
    $ winlog    <df[,16]> <data.frame[26 x 16]>
    $ ecs       <df[,1]> <data.frame[26 x 1]>
    $ host      <df[,1]> <data.frame[26 x 1]>
    $ agent     <df[,5]> <data.frame[26 x 5]>

### Шаг 2. Анализ данных

#### Задание 1. Раскройте датафрейм избавившись от вложенных датафреймов.

``` r
data <- neat_data %>%
  tidyr::unnest(c(metadata, event, log, winlog, ecs, host, agent), names_sep = ".")
data
```

    # A tibble: 101,904 × 34
       timestamp           metadata.beat metadata.type metadata.version
       <dttm>              <chr>         <chr>         <chr>           
     1 2019-10-20 20:11:06 winlogbeat    _doc          7.4.0           
     2 2019-10-20 20:11:07 winlogbeat    _doc          7.4.0           
     3 2019-10-20 20:11:09 winlogbeat    _doc          7.4.0           
     4 2019-10-20 20:11:10 winlogbeat    _doc          7.4.0           
     5 2019-10-20 20:11:11 winlogbeat    _doc          7.4.0           
     6 2019-10-20 20:11:15 winlogbeat    _doc          7.4.0           
     7 2019-10-20 20:11:15 winlogbeat    _doc          7.4.0           
     8 2019-10-20 20:11:15 winlogbeat    _doc          7.4.0           
     9 2019-10-20 20:11:15 winlogbeat    _doc          7.4.0           
    10 2019-10-20 20:11:16 winlogbeat    _doc          7.4.0           
    # ℹ 101,894 more rows
    # ℹ 30 more variables: metadata.topic <chr>, event.created <chr>,
    #   event.kind <chr>, event.code <int>, event.action <chr>, log.level <chr>,
    #   message <chr>, winlog.event_data <df[,234]>, winlog.event_id <int>,
    #   winlog.provider_name <chr>, winlog.api <chr>, winlog.record_id <int>,
    #   winlog.computer_name <chr>, winlog.process <df[,2]>,
    #   winlog.keywords <list>, winlog.provider_guid <chr>, winlog.channel <chr>, …

Использую функцию unnest() из библиотеки tidyr. Она преобразует столбцы
metadata, event, log, winlog, ecs, host и agent из neat_data в отдельные
строки. При этом используется разделитель “.” для создания новых
столбцов на основе иерархической структуры исходных столбцов.

#### Задание 2. Минимизируйте количество колонок в датафрейме – уберите колоки с единственным значением параметра.

``` r
data_minimized  <- data %>%
  select(-metadata.beat, -metadata.type, -metadata.version, -metadata.topic,
         -event.kind, -winlog.api, -agent.ephemeral_id, -agent.hostname, 
         -agent.id, -agent.version, -agent.type) %>%
  mutate(`event.created` = as.POSIXct(`event.created`, format = "%Y-%m-%dT%H:%M:%OSZ", tz = "UTC"))
data_minimized
```

    # A tibble: 101,904 × 23
       timestamp           event.created       event.code event.action     log.level
       <dttm>              <dttm>                   <int> <chr>            <chr>    
     1 2019-10-20 20:11:06 2019-10-20 20:11:09       4703 Token Right Adj… informat…
     2 2019-10-20 20:11:07 2019-10-20 20:11:09       4673 Sensitive Privi… informat…
     3 2019-10-20 20:11:09 2019-10-20 20:11:11         10 Process accesse… informat…
     4 2019-10-20 20:11:10 2019-10-20 20:11:14         10 Process accesse… informat…
     5 2019-10-20 20:11:11 2019-10-20 20:11:14         10 Process accesse… informat…
     6 2019-10-20 20:11:15 2019-10-20 20:11:18         10 Process accesse… informat…
     7 2019-10-20 20:11:15 2019-10-20 20:11:18         11 File created (r… informat…
     8 2019-10-20 20:11:15 2019-10-20 20:11:18         10 Process accesse… informat…
     9 2019-10-20 20:11:15 2019-10-20 20:11:18         10 Process accesse… informat…
    10 2019-10-20 20:11:16 2019-10-20 20:11:18         10 Process accesse… informat…
    # ℹ 101,894 more rows
    # ℹ 18 more variables: message <chr>, winlog.event_data <df[,234]>,
    #   winlog.event_id <int>, winlog.provider_name <chr>, winlog.record_id <int>,
    #   winlog.computer_name <chr>, winlog.process <df[,2]>,
    #   winlog.keywords <list>, winlog.provider_guid <chr>, winlog.channel <chr>,
    #   winlog.task <chr>, winlog.opcode <chr>, winlog.version <int>,
    #   winlog.user <df[,4]>, winlog.activity_id <chr>, …

Использую функцию select() для выбора столбцов из исходного датафрейма.
С помощью оператора “-” перед названием столбца мы указываем, что хотим
исключить указанные столбцы из результирующего датафрейма. Далее
использую функцию mutate() для создания нового столбца event.created.
Функция as.POSIXct() преобразует столбец event.created в формат POSIXct,
который является типом данных для хранения даты и времени в R. В данном
случае, используется формат даты и времени “%Y-%m-%dT%H:%M:%OSZ” и
временная зона tz = “UTC”.

#### Задание 3. Какое количество хостов представлено в данном датасете?

``` r
data_minimized %>%
  distinct(host.name)
```

    # A tibble: 1 × 1
      host.name
      <chr>    
    1 WECServer

Использую функцию distinct() для выбора уникальных значений столбца
host.name из data_minimized. Функция distinct() удаляет дублирующиеся
строки в датафрейме и возвращает только уникальные строки.

#### Задание 4. Подготовьте датафрейм с расшифровкой Windows Event_ID, приведите типы данных к типу их значений

``` r
web_url <- "https://learn.microsoft.com/en-us/windows-server/identity/ad-ds/plan/appendix-l--events-to-monitor"
web <- xml2::read_html(web_url)
event <- rvest::html_table(web)[[1]]
event
```

    # A tibble: 381 × 4
       `Current Windows Event ID` `Legacy Windows Event ID` `Potential Criticality`
       <chr>                      <chr>                     <chr>                  
     1 4618                       N/A                       High                   
     2 4649                       N/A                       High                   
     3 4719                       612                       High                   
     4 4765                       N/A                       High                   
     5 4766                       N/A                       High                   
     6 4794                       N/A                       High                   
     7 4897                       801                       High                   
     8 4964                       N/A                       High                   
     9 5124                       N/A                       High                   
    10 N/A                        550                       Medium to High         
    # ℹ 371 more rows
    # ℹ 1 more variable: `Event Summary` <chr>

``` r
event_data <- event %>%
  mutate_at(vars(`Current Windows Event ID`, `Legacy Windows Event ID`), as.integer) %>%
  rename(c(Current_Windows_Event_ID = `Current Windows Event ID`, 
           Legacy_Windows_Event_ID = `Legacy Windows Event ID`, 
           Potential_Criticality = `Potential Criticality`, 
           Event_Summary = `Event Summary`))
```

    Warning: There were 2 warnings in `mutate()`.
    The first warning was:
    ℹ In argument: `Current Windows Event ID = .Primitive("as.integer")(`Current
      Windows Event ID`)`.
    Caused by warning:
    ! в результате преобразования созданы NA
    ℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.

``` r
event_data
```

    # A tibble: 381 × 4
       Current_Windows_Event_ID Legacy_Windows_Event_ID Potential_Criticality
                          <int>                   <int> <chr>                
     1                     4618                      NA High                 
     2                     4649                      NA High                 
     3                     4719                     612 High                 
     4                     4765                      NA High                 
     5                     4766                      NA High                 
     6                     4794                      NA High                 
     7                     4897                     801 High                 
     8                     4964                      NA High                 
     9                     5124                      NA High                 
    10                       NA                     550 Medium to High       
    # ℹ 371 more rows
    # ℹ 1 more variable: Event_Summary <chr>

Использую пакеты xml2 и rvest для чтения HTML-страницы по заданному URL
и извлечения таблицы событий из HTML-кода. Затем происходит обработка и
преобразование данных таблицы, включая преобразование некоторых столбцов
в целочисленный формат и переименование столбцов для удобства работы с
данными.

#### Задание 5. Есть ли в логе события с высоким и средним уровнем значимости? Сколько их?

``` r
event_data %>%
  count(Potential_Criticality) %>%
  arrange(desc(n))
```

    # A tibble: 4 × 2
      Potential_Criticality     n
      <chr>                 <int>
    1 Low                     291
    2 Medium                   79
    3 High                      9
    4 Medium to High            2

Кол-во событий со среднем уровнем: 79, с высоким: 9.

## Оценка результатов

В ходе практической работы были получены навыки исследования данных
журнала Windows Active Directory.

## Вывод

Были закреплены навыки использования языка R для обработки данных.
